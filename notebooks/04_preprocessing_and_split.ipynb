{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8874aa1-6e73-4411-8347-3051b94c5ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape before merging: (3867, 12)\n",
      "Original class distribution:\n",
      " label\n",
      "Teff          1260\n",
      "Maize          732\n",
      "Wheat          715\n",
      "Barley         503\n",
      "Bean           253\n",
      "Pea             94\n",
      "Sorghum         72\n",
      "Dagussa         71\n",
      "Niger seed      64\n",
      "Potato          48\n",
      "Red Pepper      29\n",
      "Fallow          26\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports and load engineered data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.utils import shuffle\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Load engineered data from previous notebook\n",
    "df = pd.read_csv('../data/processed/engineered.csv')\n",
    "\n",
    "print(\"Original shape before merging:\", df.shape)\n",
    "print(\"Original class distribution:\\n\", df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5d82cac-ecb3-4547-9d6a-cf63b14c532e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distribution after first merging:\n",
      " label\n",
      "Teff               1260\n",
      "Maize               732\n",
      "Wheat               715\n",
      "Barley              503\n",
      "Pulses              347\n",
      "Minor_Cereals       143\n",
      "Other_Specialty     103\n",
      "Oilseeds             64\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: First merging of rare classes\n",
    "replace_map = {\n",
    "    'Pea': 'Pulses',\n",
    "    'Bean': 'Pulses',\n",
    "    'Niger seed': 'Oilseeds',\n",
    "    'Potato': 'Other_Specialty',\n",
    "    'Red Pepper': 'Other_Specialty',\n",
    "    'Fallow': 'Other_Specialty',\n",
    "    'Dagussa': 'Minor_Cereals',\n",
    "    'Sorghum': 'Minor_Cereals'\n",
    "}\n",
    "df['label'] = df['label'].replace(replace_map)\n",
    "\n",
    "print(\"\\nDistribution after first merging:\\n\", df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b23e2562-8242-4980-80a3-196b4a3b430e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distribution after additional merging:\n",
      " label\n",
      "Teff         1260\n",
      "Maize         732\n",
      "Wheat         715\n",
      "Barley        503\n",
      "Pulses        347\n",
      "Specialty     167\n",
      "Cereals       143\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Additional merging (group Minor_Cereals and Specialty)\n",
    "additional_map = {\n",
    "    'Minor_Cereals': 'Cereals',\n",
    "    'Oilseeds': 'Specialty',\n",
    "    'Other_Specialty': 'Specialty'\n",
    "}\n",
    "df['label'] = df['label'].replace(additional_map)\n",
    "\n",
    "print(\"\\nDistribution after additional merging:\\n\", df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1a83c51-77bd-4f2d-b8d5-9628d8f52456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distribution before augmentation:\n",
      " label\n",
      "Teff         1260\n",
      "Maize         732\n",
      "Wheat         715\n",
      "Barley        503\n",
      "Pulses        347\n",
      "Specialty     167\n",
      "Cereals       143\n",
      "Name: count, dtype: int64\n",
      "Oversampling Cereals: 143 → 400\n",
      "Oversampling Specialty: 167 → 400\n",
      "\n",
      "Distribution after augmentation:\n",
      " label\n",
      "Teff         1260\n",
      "Maize         732\n",
      "Wheat         715\n",
      "Specialty     567\n",
      "Cereals       543\n",
      "Barley        503\n",
      "Pulses        347\n",
      "Name: count, dtype: int64\n",
      "New total samples: 4667\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Manual augmentation for remaining rare classes (safe version)\n",
    "rare_threshold = 200   # Classes with <200 samples → oversample\n",
    "target_samples = 400   # Target size per rare class\n",
    "\n",
    "# Numerical columns (define explicitly - all except label)\n",
    "numeric_cols = ['N', 'P', 'K', 'ph', 'temperature', 'humidity', 'rainfall',\n",
    "                'altitude_m', 'Zn', 'S', 'soil_moisture']\n",
    "\n",
    "print(\"\\nDistribution before augmentation:\\n\", df['label'].value_counts())\n",
    "\n",
    "augmented_dfs = [df]  # Start with original data\n",
    "\n",
    "for label in df['label'].unique():\n",
    "    class_df = df[df['label'] == label].copy()\n",
    "    current_count = len(class_df)\n",
    "    \n",
    "    if current_count < rare_threshold:\n",
    "        print(f\"Oversampling {label}: {current_count} → {target_samples}\")\n",
    "        \n",
    "        # Duplicate to target\n",
    "        multiples = target_samples // current_count\n",
    "        extra = target_samples % current_count\n",
    "        \n",
    "        augmented = pd.concat([class_df] * multiples, ignore_index=True)\n",
    "        if extra > 0:\n",
    "            augmented = pd.concat([augmented, class_df.sample(n=extra, random_state=42)], ignore_index=True)\n",
    "        \n",
    "        # Safe noise addition\n",
    "        std_values = class_df[numeric_cols].std(axis=0)\n",
    "        noise_level = 0.05\n",
    "        noise = np.random.normal(0, 1, augmented[numeric_cols].shape)\n",
    "        noisy_values = augmented[numeric_cols].values + noise * std_values.values * noise_level\n",
    "        \n",
    "        augmented.loc[:, numeric_cols] = noisy_values\n",
    "        augmented.loc[:, numeric_cols] = augmented[numeric_cols].clip(lower=0)  # No negatives\n",
    "        \n",
    "        augmented_dfs.append(augmented)\n",
    "\n",
    "# Combine and shuffle\n",
    "df_aug = pd.concat(augmented_dfs, ignore_index=True)\n",
    "df_aug = shuffle(df_aug, random_state=42)\n",
    "\n",
    "print(\"\\nDistribution after augmentation:\\n\", df_aug['label'].value_counts())\n",
    "print(\"New total samples:\", len(df_aug))\n",
    "\n",
    "# Overwrite df for downstream steps\n",
    "df = df_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a2e4677-f922-4b56-bc86-8df6affea542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final class distribution (after optional major merging):\n",
      " label\n",
      "Major_Cereals    3210\n",
      "Specialty         567\n",
      "Cereals           543\n",
      "Pulses            347\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Optional - merge major cereals into one class (further balance)\n",
    "major_map = {\n",
    "    'Teff': 'Major_Cereals',\n",
    "    'Maize': 'Major_Cereals',\n",
    "    'Wheat': 'Major_Cereals',\n",
    "    'Barley': 'Major_Cereals'\n",
    "}\n",
    "df['label'] = df['label'].replace(major_map)\n",
    "\n",
    "print(\"\\nFinal class distribution (after optional major merging):\\n\", df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f35e724-b7d1-4815-8040-5dc5a9e78de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged + augmented data saved as 'engineered_merged_aug.csv'\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Save the merged/augmented dataframe\n",
    "os.makedirs('../data/processed', exist_ok=True)\n",
    "df.to_csv('../data/processed/engineered_merged_aug.csv', index=False)\n",
    "print(\"Merged + augmented data saved as 'engineered_merged_aug.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6a5957c-0a0f-4803-9b2c-c8afd503dd4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded classes: {'Cereals': np.int64(0), 'Major_Cereals': np.int64(1), 'Pulses': np.int64(2), 'Specialty': np.int64(3)}\n",
      "\n",
      "Scaled features sample:\n",
      "           N         P         K        ph  temperature  humidity  rainfall  \\\n",
      "0  0.715413 -0.071124 -0.572794 -0.819682    -0.390604  0.186867  0.758555   \n",
      "1  1.031750  0.189403  0.054317 -0.072808    -0.487255  0.023545  0.804156   \n",
      "2  0.139727 -0.246996 -0.150420 -1.192321     1.171313  1.025549  0.303969   \n",
      "3 -0.875674 -0.282722 -0.445275  0.353696    -0.548101 -0.973524 -0.587838   \n",
      "4  1.406680 -0.270132 -0.321614 -0.001724    -0.133895  0.616101  1.380241   \n",
      "\n",
      "   altitude_m        Zn         S  soil_moisture  \n",
      "0    0.088061 -0.366371  1.733051       0.294138  \n",
      "1    0.532303  0.156307 -1.173513       1.078132  \n",
      "2   -1.190368  0.641999  0.588476      -0.173013  \n",
      "3    0.860320 -0.522588 -0.444145       0.527566  \n",
      "4   -0.025905  0.563644  0.121116       1.628699  \n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Separate X/y, encode label, scale features\n",
    "X = df.drop('label', axis=1)\n",
    "y = df['label']\n",
    "\n",
    "# Encode target\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "joblib.dump(le, '../models/label_encoder_merged.pkl')\n",
    "print(\"Encoded classes:\", dict(zip(le.classes_, le.transform(le.classes_))))\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "joblib.dump(scaler, '../models/scaler_merged.pkl')\n",
    "\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "print(\"\\nScaled features sample:\\n\", X_scaled_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f895d0ee-491a-406e-a5f5-0e0f387f9c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Split sizes:\n",
      "Train: 3266 | Val: 700 | Test: 701\n",
      "All merged files saved with '_merged' suffix!\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Stratified splits and save everything\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X_scaled, y_encoded, test_size=0.3, random_state=42, stratify=y_encoded\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(\"\\nSplit sizes:\")\n",
    "print(f\"Train: {X_train.shape[0]} | Val: {X_val.shape[0]} | Test: {X_test.shape[0]}\")\n",
    "\n",
    "# Save X splits\n",
    "pd.DataFrame(X_train, columns=X.columns).to_csv('../data/processed/X_train_merged.csv', index=False)\n",
    "pd.DataFrame(X_val, columns=X.columns).to_csv('../data/processed/X_val_merged.csv', index=False)\n",
    "pd.DataFrame(X_test, columns=X.columns).to_csv('../data/processed/X_test_merged.csv', index=False)\n",
    "\n",
    "# Save y encoded\n",
    "pd.DataFrame(y_train, columns=['label_encoded']).to_csv('../data/processed/y_train_merged.csv', index=False)\n",
    "pd.DataFrame(y_val, columns=['label_encoded']).to_csv('../data/processed/y_val_merged.csv', index=False)\n",
    "pd.DataFrame(y_test, columns=['label_encoded']).to_csv('../data/processed/y_test_merged.csv', index=False)\n",
    "\n",
    "# Save human-readable crop names\n",
    "pd.DataFrame(le.inverse_transform(y_train), columns=['crop']).to_csv('../data/processed/y_train_names_merged.csv', index=False)\n",
    "pd.DataFrame(le.inverse_transform(y_val), columns=['crop']).to_csv('../data/processed/y_val_names_merged.csv', index=False)\n",
    "pd.DataFrame(le.inverse_transform(y_test), columns=['crop']).to_csv('../data/processed/y_test_names_merged.csv', index=False)\n",
    "\n",
    "print(\"All merged files saved with '_merged' suffix!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366bef61-d3a7-4dee-9da7-282e88a0d796",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
